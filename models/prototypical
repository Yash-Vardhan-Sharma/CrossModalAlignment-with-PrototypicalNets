import argparse
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader

import learn2learn as l2l
from learn2learn.data.transforms import NWays, KShots, LoadData, RemapLabels


def pairwise_distances_logits(a, b):
    n = a.shape[0]
    m = b.shape[0]
    # print('x',a.shape)
    # print('y',b.shape)
    logits = -((a.unsqueeze(1).expand(n, m, -1) -
                b.unsqueeze(0).expand(n, m, -1))**2).sum(dim=2)
    return logits


def accuracy(predictions, targets):
    predictions = predictions.argmax(dim=1).view(targets.shape)
    return (predictions == targets).sum().float() / targets.size(0)


class Convnet(nn.Module):

    def __init__(self, x_dim=1, hid_dim=64, z_dim=64):
        super().__init__()
        self.encoder = l2l.vision.models.ConvBase(output_size=z_dim,
                                                  hidden=hid_dim,
                                                  channels=x_dim)
        self.out_channels = 1600

    def forward(self, x):
        x = self.encoder(x)
        return x.view(x.size(0), -1)


def fast_adapt(model, batch, ways, shot, metric=None, device=None):
    if metric is None:
        metric = pairwise_distances_logits
    if device is None:
        device = model.device()
    data, labels = batch
    data = data.to(device)
    labels = labels.to(device)
    n_items = shot * ways

    # Sort data samples by labels
    # TODO: Can this be replaced by ConsecutiveLabels ?
    sort = torch.sort(labels)
    data = data.squeeze(0)[sort.indices].squeeze(0)
    labels = labels.squeeze(0)[sort.indices].squeeze(0)

    # Compute support and query embeddings
    embeddings = model(data)
    support_indices = np.zeros(data.size(0), dtype=bool)
    selection = np.arange(ways) * (shot*2)

    for offset in range(shot):
        support_indices[selection + offset] = True

    query_indices = torch.from_numpy(~support_indices)
    support_indices = torch.from_numpy(support_indices)
    support = embeddings[support_indices]
    triplet_loss=0
    for n in range(ways):
      a=list(range(ways))
      a.remove(n)
      b = random.choice(a)
      x=support.reshape(ways,shot,-1)[n][random.randint(0,shot-1)]
      matrix = support.reshape(ways,shot,-1)
      triplet_loss += ((matrix[n][random.randint(0,shot-1)]-matrix[b][random.randint(0,shot-1)])**2).mean() - ((matrix[n][random.randint(0,shot-1)]-matrix[n][random.randint(0,shot-1)])**2).mean()

    triplet_loss/=ways
    support = support.reshape(ways, shot, -1).mean(dim=1)

    query = embeddings[query_indices]
    labels = labels[query_indices].long()

    logits = pairwise_distances_logits(query, support)
    loss = F.cross_entropy(logits, labels)
    # print(20*triplet_loss)
    # print(loss)
    acc = accuracy(logits, labels)
    return loss-20*triplet_loss, acc

model = Convnet()
train_loader , valid_loader , test_loader = load_omniglot(ways=5 , shots = 5)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--max-epoch', type=int, default=20000)
    parser.add_argument('--shot', type=int, default=5)
    parser.add_argument('--way', type=int, default=5)
    parser.add_argument('--gpu', default=1)
    parser.add_argument('-f')
    args = parser.parse_args()
    print(args)

    device = torch.device('cpu')
    if args.gpu and torch.cuda.device_count():
        print("Using gpu")
        torch.cuda.manual_seed(43)
        device = torch.device('cuda')

    model.to(device)


    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    # lr_scheduler = torch.optim.lr_scheduler.StepLR(
    #     optimizer, step_size=20, gamma=0.5)

    for epoch in range(1, args.max_epoch + 1):
        model.train()

        loss_ctr = 0
        n_loss = 0
        n_acc = 0

        for i in range(1):
            batch = train_loader.sample()

            loss, acc = fast_adapt(model,
                                   batch,
                                   args.way,
                                   args.shot,
                                  #  args.train_query,
                                   metric=pairwise_distances_logits,
                                   device=device)

            loss_ctr += 1
            n_loss += loss
            n_acc += acc

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        # lr_scheduler.step()

        print('epoch {}, train, loss={:.4f} acc={:.4f}'.format(
            epoch, n_loss.item()/loss_ctr, n_acc/loss_ctr))

        model.eval()

        loss_ctr = 0
        n_loss = 0
        n_acc = 0
        for i, batch in enumerate(valid_loader):
            loss, acc = fast_adapt(model,
                                   batch,
                                   args.way,
                                   args.shot,
                                  #  args.test_query,
                                   metric=pairwise_distances_logits,
                                   device=device)

            loss_ctr += 1
            n_loss += loss.item()
            n_acc += acc

            if(i>5):
              break

        print('epoch {}, val, loss={:.4f} acc={:.4f}'.format(
            epoch, n_loss/loss_ctr, n_acc/loss_ctr))

    loss_ctr = 0
    n_acc = 0

    for i, batch in enumerate(test_loader, 1):
        loss, acc = fast_adapt(model,
                               batch,
                               args.way,
                               args.shot,
                               metric=pairwise_distances_logits,
                               device=device)
        loss_ctr += 1
        n_acc += acc
        print('batch {}: {:.2f}({:.2f})'.format(
            i, n_acc/loss_ctr * 100, acc * 100))
